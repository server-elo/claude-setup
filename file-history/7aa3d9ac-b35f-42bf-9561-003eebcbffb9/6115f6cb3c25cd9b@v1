# LLM Red-Team CI

![LLM Red-Team CI](https://img.shields.io/badge/LLM-Red--Team%20CI-red)
![License](https://img.shields.io/badge/license-MIT-blue)
![Version](https://img.shields.io/badge/version-1.0.0-green)

Automatically attack your customers' LLM apps in CI to catch prompt-injection, data exfiltration, jailbreaks, and tool abuse before deployment.

## Overview

LLM Red-Team CI is a GitHub Action that runs a corpus of adversarial prompts against your LLM application's staging endpoint. It uses a "judge" model to score failures and emits SARIF/JSON reports, along with Slack notifications and simple HTML reports containing reproducible curl commands.

### Key Features

- **Automated Security Testing**: Test your LLM applications against a comprehensive corpus of adversarial prompts
- **Vulnerability Detection**: Catch prompt-injection, data exfiltration, jailbreaks, and tool abuse
- **CI/CD Integration**: Run tests automatically in your CI/CD pipeline
- **Detailed Reports**: Get SARIF, JSON, and HTML reports with reproducible curl commands
- **Slack Notifications**: Receive instant alerts when vulnerabilities are detected
- **Customizable Testing**: Configure test categories, intensity, and more

## Quick Start

### 1. Add the GitHub Action to your workflow

Create a file at `.github/workflows/llm-redteam.yml` with the following content:

```yaml
name: LLM Security Testing

on:
  pull_request:
    branches: [ main, master ]
  workflow_dispatch:

jobs:
  llm-redteam:
    runs-on: ubuntu-latest
    name: LLM Red-Team CI
    steps:
      - name: Run LLM Red-Team Tests
        uses: llm-redteam/llm-redteam-ci@v1
        with:
          api_endpoint: ${{ secrets.LLM_API_ENDPOINT }}
          api_key: ${{ secrets.LLM_API_KEY }}
          test_categories: 'prompt-injection,data-exfiltration,jailbreak,tool-abuse'
          test_intensity: 'medium'
          slack_webhook: ${{ secrets.SLACK_WEBHOOK }}
          report_format: 'all'
          service_tier: 'standard'
```

### 2. Configure your secrets

In your GitHub repository, go to Settings > Secrets and add the following secrets:

- `LLM_API_ENDPOINT`: The endpoint URL of your LLM application
- `LLM_API_KEY`: API key for authentication with your LLM application (if required)
- `SLACK_WEBHOOK`: Slack webhook URL for notifications (optional)

### 3. Run the workflow

The workflow will run automatically on pull requests to your main/master branch. You can also run it manually from the Actions tab in your GitHub repository.

## Configuration Options

| Option | Description | Required | Default |
|--------|-------------|----------|---------|
| `api_endpoint` | The endpoint URL of the LLM application to test | Yes | - |
| `api_key` | API key for authentication with the LLM application | No | - |
| `test_categories` | Categories of tests to run (comma-separated) | No | `all` |
| `test_intensity` | Intensity level of testing (low, medium, high) | No | `medium` |
| `slack_webhook` | Slack webhook URL for notifications | No | - |
| `report_format` | Format of the test report (sarif, json, html, all) | No | `all` |
| `service_tier` | Service tier for test volume (basic, standard, premium) | No | `standard` |

## Test Categories

LLM Red-Team CI tests for the following categories of vulnerabilities:

### Prompt Injection

Tests that attempt to manipulate the LLM into ignoring its system prompt or revealing sensitive information about its configuration.

Examples:
- Direct instruction override attempts
- Role-play override attempts
- Security test pretense
- Authority impersonation

### Data Exfiltration

Tests that attempt to make the LLM extract and transmit sensitive information.

Examples:
- External data exfiltration
- Sensitive data memorization
- Data encoding exfiltration
- Information extraction instructions

### Jailbreaks

Tests that attempt to bypass the LLM's safety measures and content policies.

Examples:
- Academic pretense for harmful content
- Persona switching
- Creative completion exploits
- Translation bypass techniques

### Tool Abuse

Tests that attempt to misuse tools or plugins that the LLM might have access to.

Examples:
- Unauthorized file access
- Malicious code execution
- Internal network access
- Destructive database operations

## Reports and Notifications

### SARIF Reports

SARIF (Static Analysis Results Interchange Format) reports are generated for integration with GitHub Code Scanning and other security tools.

### JSON Reports

Detailed JSON reports contain all test results, including prompts, responses, and vulnerability assessments.

### HTML Reports

User-friendly HTML reports include:
- Summary of test results
- Detailed vulnerability information
- Reproducible curl commands for each vulnerability
- Evidence and severity ratings

### Slack Notifications

Slack notifications include:
- Summary of test results
- List of detected vulnerabilities
- Links to detailed reports

## Pricing

LLM Red-Team CI offers flexible pricing based on your testing needs:

| Plan | Price | Tests per Month |
|------|-------|-----------------|
| Basic | $200/mo | 1,000 |
| Standard | $500/mo | 5,000 |
| Premium | $1,000/mo | 20,000 |

See our [pricing documentation](docs/pricing.md) for detailed information.

## Architecture

LLM Red-Team CI consists of the following components:

- **GitHub Action**: Runs in your CI/CD pipeline
- **Test Runner**: Executes adversarial prompts against your LLM application
- **Judge Model**: Evaluates responses using Claude to detect vulnerabilities
- **Report Generator**: Creates SARIF, JSON, and HTML reports
- **Notification System**: Sends alerts to Slack
- **API**: Cloudflare Workers API for test management and result retrieval
- **Database**: Supabase/Postgres for storing test results and corpus data

## Cost Guardrails

To prevent runaway costs, LLM Red-Team CI includes several built-in guardrails:

- **Budget defaults**: max_seconds=45, max_tokens=2k, max_concurrency=4
- **Judge cost SLO**: <$0.05/test p95. System will alert if rolling 1h average exceeds threshold
- **Caching**: Deterministic verdicts are cached by (attack_id, model, seed, judge_version) for 24h in staging to cap cost during testing flurries
- **Concurrency limits**: Prevents resource exhaustion during high-volume testing
- **Time budgets**: Enforces maximum execution time per test run

## CI/CD Integration Features

### Automatic PR Comments
The system automatically posts detailed results to your pull requests, including:
- Summary of findings
- Worst severity detected
- Top failure types
- Examples of vulnerabilities with evidence

### TTL-Based Muting
Unlike permanent ignores, all muted tests have automatic expiration dates:
- Mutes expire after specified TTL (Time To Live)
- Expired mutes cause build failures to force cleanup
- Supports severity-based muting (mute up to critical severity)

### Automatic Issue Creation
High and critical severity findings automatically create tickets in your issue tracker:
- Jira integration with detailed vulnerability information
- Linear API support (contact us for configuration)

### SIEM Integration
Export findings to your security infrastructure:
- Splunk HEC support with structured events
- Datadog logs integration
- Custom export formats available

## Development

### Prerequisites

- Node.js 20.x
- Python 3.10+
- Supabase account (for database)
- Anthropic API key (for judge model)

### Setup

1. Clone the repository:
```bash
git clone https://github.com/llm-redteam/llm-redteam-ci.git
cd llm-redteam-ci
```

2. Install dependencies:
```bash
pip install -r src/github-action/requirements.txt
```

3. Set up environment variables:
```bash
export ANTHROPIC_API_KEY=your_anthropic_api_key
export SUPABASE_URL=your_supabase_url
export SUPABASE_KEY=your_supabase_key
```

### Running Tests

```bash
python tests/run_tests.py
```

## Contributing

We welcome contributions to LLM Red-Team CI! Please see our [contributing guidelines](CONTRIBUTING.md) for more information.

## License

LLM Red-Team CI is licensed under the MIT License. See [LICENSE](LICENSE) for more information.

## Contact

- Website: [llm-redteam-ci.com](https://llm-redteam-ci.com)
- Email: contact@llm-redteam-ci.com
- Twitter: [@LLMRedTeamCI](https://twitter.com/LLMRedTeamCI)