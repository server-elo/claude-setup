"""
OPTIMIZED Speech-to-Text with faster-whisper
4x faster than standard OpenAI Whisper
Target: <500ms latency (WhisperKit paper)
"""
import numpy as np
from loguru import logger


class OptimizedSpeechToText:
    """
    Optimized STT using faster-whisper (CTranslate2 backend)

    Performance: 4x faster than openai-whisper
    Target latency: <500ms (WhisperKit: 460ms on-device)
    """

    def __init__(self, language="de", model_size="base"):
        """
        Initialize faster-whisper model

        Args:
            language: Language code ('de' for German, 'en' for English)
            model_size: Model size ('tiny', 'base', 'small', 'medium', 'large')
                       base is good balance of speed/accuracy
        """
        logger.info(f"Loading faster-whisper model ({model_size})...")

        try:
            from faster_whisper import WhisperModel

            # Load model with CTranslate2 backend (4x faster)
            self.model = WhisperModel(
                model_size,
                device="cpu",  # Use "cuda" if GPU available
                compute_type="int8"  # Quantized for speed
            )

            self.language = language

            logger.success(f"✅ faster-whisper loaded ({model_size}, language: {language})")
            logger.info("   Expected: 4x faster than standard Whisper, <500ms latency")

        except ImportError as e:
            logger.error(f"Failed to load faster-whisper: {e}")
            logger.info("Install with: pip install faster-whisper")
            raise

    def transcribe(self, audio_data) -> str:
        """
        Convert audio bytes to text (4x faster than openai-whisper)

        Returns:
            str: Transcribed text
        """
        try:
            # Convert bytes to numpy array
            if isinstance(audio_data, bytes):
                logger.debug(f"Received {len(audio_data)} bytes")
                audio_np = np.frombuffer(audio_data, dtype=np.int16).astype(np.float32) / 32768.0
            else:
                audio_np = audio_data.astype(np.float32) / 32768.0

            logger.debug(f"Audio: {len(audio_np)} samples, power={np.abs(audio_np).mean():.3f}")

            # Check if audio is too quiet (likely silence/noise)
            audio_power = np.abs(audio_np).mean()
            if audio_power < 0.01:
                logger.warning(f"Audio too quiet (power={audio_power:.4f}) - likely silence")
                return ""

            # Transcribe with faster-whisper (streaming)
            segments, info = self.model.transcribe(
                audio_np,
                language=self.language,
                beam_size=1,              # Faster than default (5)
                vad_filter=False,         # Disable VAD (already using WebRTC VAD)
                word_timestamps=False,    # Faster without timestamps
                condition_on_previous_text=False  # Don't use context (faster)
            )

            # Collect all segments
            segment_list = list(segments)
            logger.debug(f"Whisper returned {len(segment_list)} segments")

            if len(segment_list) == 0:
                logger.warning("Whisper returned 0 segments - audio may be unclear")
                return ""

            transcript = " ".join([segment.text for segment in segment_list]).strip()

            logger.info(f"📝 Transcribed ({len(audio_np)} samples, power={audio_power:.3f}): '{transcript}'")
            return transcript

        except Exception as e:
            logger.error(f"STT error: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return ""

    def transcribe_stream(self, audio_stream):
        """
        Stream transcription with incremental decoding
        Inspired by WhisperKit on-device optimization

        Yields partial transcripts as audio arrives
        """
        accumulated_audio = []

        for audio_chunk in audio_stream:
            accumulated_audio.append(audio_chunk)

            # Process every ~1 second of audio
            if len(accumulated_audio) >= 16:  # ~1 second at 16kHz
                combined_audio = b''.join(accumulated_audio)
                text = self.transcribe(combined_audio)
                if text:
                    yield text
                accumulated_audio = []

        # Final processing
        if accumulated_audio:
            combined_audio = b''.join(accumulated_audio)
            text = self.transcribe(combined_audio)
            if text:
                yield text
