"""
OPTIMIZED Console Handler - Research-Backed Implementation
Combines insights from 4 arXiv papers:
- VoXtream: <150ms TTS first-packet
- WhisperKit: <500ms ASR latency
- Voila: <200ms end-to-end response
- TTS-1: Real-time 48kHz synthesis

Target: <1 second end-to-end latency (vs current ~3-5s)
"""
import pyaudio
import wave
import io
import sys
import os
import threading
import queue
import webrtcvad
import collections
import time
from loguru import logger

# Import optimized components
from .stt_optimized import OptimizedSpeechToText
from .tts_piper_local import PiperLocalTTS  # MOSHI-INSPIRED: 100% local TTS
from .llm import LanguageModel


class OptimizedConsoleHandler:
    """
    Research-backed ultra-low latency voice handler

    Performance Targets (from papers):
    - ASR: <500ms (WhisperKit: 460ms)
    - TTS First-Packet: <150ms (VoXtream: 102ms)
    - End-to-End: <1000ms (Voila: 195ms + margin)

    Architecture:
    - 5 parallel threads for pipeline overlap
    - Streaming ASR with faster-whisper (4x speedup)
    - VoXtream-style incremental TTS
    - Sentence-by-sentence generation
    - Full VAD with automatic speech detection
    """

    def __init__(self, llm_model: str = "gemma2:2b", language: str = "de"):
        """
        Initialize optimized handler

        Args:
            llm_model: Ollama model (gemma2:2b recommended for speed)
            language: 'de' for German, 'en' for English
        """
        logger.info("ðŸš€ Initializing OPTIMIZED handler (research-backed)...")

        # Optimized components
        self.stt = OptimizedSpeechToText(language=language, model_size="base")
        # MOSHI-INSPIRED: Local TTS with full path to voice model
        voice_path = os.path.expanduser("~/.local/share/piper-voices/de_DE-thorsten-medium")
        self.tts = PiperLocalTTS(voice_model=voice_path)
        self.llm = LanguageModel(model=llm_model)

        # Audio settings - optimized for minimum latency
        self.CHUNK = 480  # 30ms at 16kHz
        self.FORMAT = pyaudio.paInt16
        self.CHANNELS = 1
        self.RATE = 16000

        # VAD for speech detection
        self.vad = webrtcvad.Vad(3)  # Most aggressive

        # Streaming queues for pipeline overlap
        self.audio_chunks_queue = queue.Queue()  # Raw audio chunks
        self.stt_queue = queue.Queue()           # Transcriptions
        self.llm_token_queue = queue.Queue()     # LLM tokens
        self.tts_audio_queue = queue.Queue()     # TTS audio chunks

        # Control flags
        self.is_listening = False
        self.is_speaking = False  # Disable mic during AI speech
        self.interrupt_speech = False

        # Performance tracking (paper metrics)
        self.latency_stats = {
            'speech_detected': 0,
            'stt_complete': 0,      # WhisperKit target: <500ms
            'llm_first_token': 0,
            'tts_first_packet': 0,  # VoXtream target: <150ms
            'audio_play_start': 0,
            'total_response': 0     # Voila target: <200ms (we aim <1000ms)
        }

        self.pyaudio = pyaudio.PyAudio()

        logger.success("âœ… OPTIMIZED handler ready")
        logger.info("ðŸ“Š Performance targets:")
        logger.info("   - ASR latency: <500ms (WhisperKit)")
        logger.info("   - TTS first-packet: <150ms (VoXtream)")
        logger.info("   - Total response: <1000ms (Voila-inspired)")

    def is_speech(self, audio_chunk: bytes) -> bool:
        """VAD speech detection"""
        try:
            return self.vad.is_speech(audio_chunk, self.RATE)
        except:
            return False

    def continuous_audio_capture(self):
        """THREAD 1: Continuously capture audio with VAD"""
        stream = self.pyaudio.open(
            format=self.FORMAT,
            channels=self.CHANNELS,
            rate=self.RATE,
            input=True,
            frames_per_buffer=self.CHUNK
        )

        ring_buffer = collections.deque(maxlen=30)
        speech_active = False
        silence_count = 0
        silence_threshold = 33  # ~1 second

        try:
            while self.is_listening:
                # Skip audio capture when AI is speaking
                if self.is_speaking:
                    time.sleep(0.01)
                    continue

                chunk = stream.read(self.CHUNK, exception_on_overflow=False)
                is_speech_chunk = self.is_speech(chunk)

                if not speech_active:
                    ring_buffer.append((chunk, is_speech_chunk))
                    if is_speech_chunk and not self.is_speaking:
                        logger.info("ðŸŽ¤ Speech detected")
                        self.latency_stats['speech_detected'] = time.time()
                        speech_active = True
                        # Send buffered audio
                        for buffered_chunk, _ in ring_buffer:
                            self.audio_chunks_queue.put(buffered_chunk)
                        self.audio_chunks_queue.put(chunk)
                        silence_count = 0
                else:
                    self.audio_chunks_queue.put(chunk)

                    if is_speech_chunk:
                        silence_count = 0
                    else:
                        silence_count += 1

                    if silence_count >= silence_threshold:
                        logger.info("Speech ended")
                        self.audio_chunks_queue.put(None)  # End marker
                        speech_active = False
                        silence_count = 0
                        ring_buffer.clear()

        finally:
            stream.stop_stream()
            stream.close()

    def streaming_stt_processor(self):
        """
        THREAD 2: Optimized STT with faster-whisper
        Target: <500ms latency (WhisperKit paper)
        """
        import numpy as np

        accumulated_audio = []

        while self.is_listening:
            try:
                chunk = self.audio_chunks_queue.get(timeout=0.1)

                if chunk is None:  # End of speech
                    if accumulated_audio:
                        stt_start = time.time()

                        # Combine all audio chunks
                        combined_audio = b''.join(accumulated_audio)

                        # Transcribe with faster-whisper (4x speedup)
                        final_text = self.stt.transcribe(combined_audio)

                        stt_duration = (time.time() - stt_start) * 1000

                        if final_text:
                            self.stt_queue.put(('FINAL', final_text))
                            self.latency_stats['stt_complete'] = time.time()

                            logger.info(f"ðŸ“ Transcript: {final_text}")
                            logger.success(f"âš¡ STT latency: {stt_duration:.0f}ms (target: <500ms)")

                    accumulated_audio = []
                    continue

                # Accumulate audio chunks
                accumulated_audio.append(chunk)

            except queue.Empty:
                continue
            except Exception as e:
                logger.error(f"STT error: {e}")

    def streaming_llm_processor(self, system_prompt: str):
        """
        THREAD 3: Streaming LLM with sentence-level dispatch
        Starts TTS immediately on first sentence
        """
        from ollama import chat as ollama_chat

        while self.is_listening:
            try:
                marker, transcript = self.stt_queue.get(timeout=0.1)

                if marker == 'FINAL':
                    print(f"\nSie: {transcript}")

                    # Check for exit
                    if any(word in transcript.lower() for word in ["auf wiedersehen", "tschÃ¼ss", "beenden"]):
                        self.stt_queue.put(('EXIT', transcript))
                        break

                    messages = [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": transcript}
                    ]

                    sentence_buffer = ""
                    print("Sofia: ", end="", flush=True)

                    for chunk in ollama_chat(
                        model=self.llm.model,
                        messages=messages,
                        stream=True,
                        options={"num_predict": 150, "temperature": 0.7}
                    ):
                        if "message" in chunk and "content" in chunk["message"]:
                            token = chunk["message"]["content"]
                            sentence_buffer += token
                            print(token, end="", flush=True)

                            if not self.latency_stats.get('llm_first_token'):
                                self.latency_stats['llm_first_token'] = time.time()

                            # Dispatch on sentence boundaries (VoXtream strategy)
                            if any(p in token for p in ['.', '!', '?']):
                                if sentence_buffer.strip():
                                    self.llm_token_queue.put(('SENTENCE', sentence_buffer.strip()))
                                    sentence_buffer = ""

                    if sentence_buffer.strip():
                        self.llm_token_queue.put(('SENTENCE', sentence_buffer.strip()))

                    self.llm_token_queue.put(('DONE', None))
                    print()

            except queue.Empty:
                continue
            except Exception as e:
                logger.error(f"LLM error: {e}")

    def streaming_tts_processor(self):
        """
        THREAD 4: VoXtream-style streaming TTS
        Target: <150ms first-packet latency
        """
        while self.is_listening:
            try:
                marker, text = self.llm_token_queue.get(timeout=0.1)

                if marker == 'SENTENCE':
                    tts_start = time.time()

                    # VoXtream-style sentence generation
                    for audio_chunk in self.tts.speak_stream_sentences(text):
                        if audio_chunk:
                            self.tts_audio_queue.put(audio_chunk)

                            if not self.latency_stats.get('tts_first_packet'):
                                tts_latency = (time.time() - tts_start) * 1000
                                self.latency_stats['tts_first_packet'] = time.time()
                                logger.success(f"ðŸŽ¯ TTS first-packet: {tts_latency:.0f}ms (target: <150ms)")

                elif marker == 'DONE':
                    self.tts_audio_queue.put(None)

            except queue.Empty:
                continue
            except Exception as e:
                logger.error(f"TTS error: {e}")

    def streaming_audio_player(self):
        """THREAD 5: Instant audio playback"""
        stream = self.pyaudio.open(
            format=self.FORMAT,
            channels=self.CHANNELS,
            rate=24000,  # Match Edge TTS output rate
            output=True,
            frames_per_buffer=self.CHUNK
        )

        try:
            while self.is_listening:
                try:
                    audio_data = self.tts_audio_queue.get(timeout=0.1)

                    if audio_data is None:
                        self.is_speaking = False
                        self.print_latency_stats()
                        continue

                    if not self.latency_stats.get('audio_play_start'):
                        self.latency_stats['audio_play_start'] = time.time()
                        self.latency_stats['total_response'] = (
                            self.latency_stats['audio_play_start'] -
                            self.latency_stats['speech_detected']
                        ) * 1000

                    # Disable microphone while speaking
                    self.is_speaking = True

                    # Play audio
                    chunk_size = self.CHUNK * 2
                    for i in range(0, len(audio_data), chunk_size):
                        if self.interrupt_speech:
                            break
                        chunk = audio_data[i:i + chunk_size]
                        try:
                            stream.write(chunk)
                        except Exception as e:
                            logger.error(f"Audio playback error: {e}")
                            break

                    # Keep mic disabled for 200ms after audio (echo buffer)
                    time.sleep(0.2)
                    self.is_speaking = False

                except queue.Empty:
                    continue

        finally:
            stream.stop_stream()
            stream.close()

    def print_latency_stats(self):
        """Print research-backed latency breakdown"""
        if not self.latency_stats.get('speech_detected'):
            return

        base = self.latency_stats['speech_detected']

        print("\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
        print("â•‘           PERFORMANCE METRICS (Research-Backed)       â•‘")
        print("â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£")

        if self.latency_stats.get('stt_complete'):
            stt_latency = (self.latency_stats['stt_complete'] - base) * 1000
            target_status = "âœ…" if stt_latency < 500 else "âš ï¸"
            print(f"â•‘ ASR Latency:        {stt_latency:6.0f}ms {target_status} (target: <500ms)  â•‘")

        if self.latency_stats.get('llm_first_token'):
            llm_latency = (self.latency_stats['llm_first_token'] - self.latency_stats.get('stt_complete', base)) * 1000
            print(f"â•‘ LLM Processing:     {llm_latency:6.0f}ms                      â•‘")

        if self.latency_stats.get('tts_first_packet'):
            tts_latency = (self.latency_stats['tts_first_packet'] - self.latency_stats.get('llm_first_token', base)) * 1000
            target_status = "âœ…" if tts_latency < 150 else "âš ï¸"
            print(f"â•‘ TTS First-Packet:   {tts_latency:6.0f}ms {target_status} (target: <150ms)  â•‘")

        if self.latency_stats.get('total_response'):
            total = self.latency_stats['total_response']
            target_status = "âœ…" if total < 1000 else "ðŸŽ¯" if total < 2000 else "âš ï¸"
            print(f"â•‘ TOTAL RESPONSE:     {total:6.0f}ms {target_status} (target: <1000ms) â•‘")

        print("â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£")
        print("â•‘ Baseline (pre-optimization): ~3000-5000ms             â•‘")
        print("â•‘ Paper benchmarks: WhisperKit 460ms, VoXtream 102ms    â•‘")
        print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")

        # Reset stats
        self.latency_stats = {k: 0 for k in self.latency_stats}

    def run_conversation(self, system_prompt: str):
        """Run optimized streaming conversation"""
        print("\n" + "=" * 60)
        print("SOFIA - OPTIMIZED MODE (Research-Backed)")
        print("=" * 60)
        print("ðŸš€ Optimizations:")
        print("   âœ… faster-whisper (4x speedup)")
        print("   âœ… VoXtream-style streaming TTS")
        print("   âœ… Sentence-by-sentence generation")
        print("   âœ… 5-thread parallel pipeline")
        print("   âœ… Automatic VAD speech detection")
        print("\nðŸ“Š Performance Targets:")
        print("   - ASR: <500ms (WhisperKit paper)")
        print("   - TTS: <150ms first-packet (VoXtream paper)")
        print("   - Total: <1000ms (Voila-inspired)")
        print("=" * 60)
        print("\nðŸŽ¤ Ready! Just speak... (Say 'Auf Wiedersehen' to end)\n")

        self.is_listening = True

        # Start all parallel threads
        threads = [
            threading.Thread(target=self.continuous_audio_capture, daemon=True),
            threading.Thread(target=self.streaming_stt_processor, daemon=True),
            threading.Thread(target=self.streaming_llm_processor, args=(system_prompt,), daemon=True),
            threading.Thread(target=self.streaming_tts_processor, daemon=True),
            threading.Thread(target=self.streaming_audio_player, daemon=True),
        ]

        for t in threads:
            t.start()

        try:
            # Main thread monitors for exit
            while self.is_listening:
                time.sleep(0.1)

        except KeyboardInterrupt:
            print("\n\nðŸ‘‹ Auf Wiedersehen!")
        finally:
            self.is_listening = False
            time.sleep(0.5)

    def cleanup(self):
        """Clean up resources"""
        self.is_listening = False
        self.pyaudio.terminate()
        logger.info("Optimized handler cleaned up")
